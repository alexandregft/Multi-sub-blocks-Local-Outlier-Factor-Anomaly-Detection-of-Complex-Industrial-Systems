{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "731f2f9f-95b1-484a-84b3-e6a965948dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymit\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7042544-ca1d-45e4-b0de-1835975f9864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_name=\"../Results/TEP\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b48e76a-7056-49f4-b6c1-a02caa2d680d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# open the file in read binary mode\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# load the data from the file using pickle\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     data_list \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "file_name = \"Global_res_TEP_LOF_0.9_True_True.pkl\"\n",
    "file_name = path_name + \"/\" + file_name\n",
    "\n",
    "# open the file in read binary mode\n",
    "with open(file_name, \"rb\") as f:\n",
    "    # load the data from the file using pickle\n",
    "    data_list = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26ff0063-54d3-45c0-bc9f-46d030b940d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOF\n",
      "GHotelling.csv\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     column_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOF_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpercentage_coreset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnormalised\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmultiblock\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHotelling\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m filename:\n\u001b[0;32m---> 24\u001b[0m     normalised \u001b[38;5;241m=\u001b[39m \u001b[43mfilename\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     25\u001b[0m     column_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHotelling_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnormalised\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Define the directory where the CSV files are stored\n",
    "directory = 'Article_journal_Distributed/Results/TEP'\n",
    "\n",
    "# Initialize an empty dictionary to store the dataframes\n",
    "dataframes = {}\n",
    "\n",
    "# Loop through the files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\") and 'Ratio_faults_found_df' in filename:\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # Extract the model name and other parameters from the filename\n",
    "        model_name = filename.split('_')[5]\n",
    "        print(model_name)\n",
    "        if 'LOF' in filename:\n",
    "            percentage_coreset = filename.split('_')[6]\n",
    "            normalised = filename.split('_')[7]\n",
    "            multiblock = filename.split('_')[8].split('.')[0]\n",
    "            column_name = f\"LOF_{percentage_coreset}_{normalised}_{multiblock}\"\n",
    "            \n",
    "        elif 'Hotelling' in filename:\n",
    "            normalised = filename.split('_')[6].split('.')[0]\n",
    "            column_name = f\"Hotelling_{normalised}\"\n",
    "\n",
    "            \n",
    "        else:\n",
    "            model_name=model_name.split(\".\")[0]\n",
    "            column_name = f\"{model_name}\"\n",
    "        # Read the CSV file into a dataframe and add it to the dictionary\n",
    "        df = pd.read_csv(filepath, index_col=0)\n",
    "        dataframes[column_name] = df\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "result = pd.concat(dataframes.values(), axis=1, keys=dataframes.keys())\n",
    "\n",
    "# Print the concatenated dataframe\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd534587-0de7-4c1e-ae3b-e0d71b2a06da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uic71221\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d4d03b-2523-461f-9f01-199ffd593023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
